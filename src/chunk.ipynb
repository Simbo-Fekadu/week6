{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3abacc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 825338 chunks\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('../data/filtered_complaints.csv')\n",
    "\n",
    "# Split into chunks\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "chunks = splitter.create_documents(data['clean_text'].tolist())  # Define chunks here\n",
    "print(f\"Created {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecd3fd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25792/25792 [35:01<00:00, 12.27it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (825338, 384)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize embedding model\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight but effective\n",
    "\n",
    "# Embed chunks\n",
    "chunk_texts = [chunk.page_content for chunk in chunks]\n",
    "embeddings = embedder.encode(chunk_texts, show_progress_bar=True)\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")  # (num_chunks, 384)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fd8bcdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created fresh directory at ../vector_store\n",
      "Initializing Chroma client with path: c:\\Users\\Simbo\\Desktop\\week6\\week6\\vector_store\n",
      "Successfully created or retrieved 'complaints' collection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 825338/825338 [35:58<00:00, 382.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Completed 826 batches. Vector store ready!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# Verify variables exist\n",
    "assert all(var in globals() for var in ['chunks', 'embeddings']), \"Run Cells 1 & 2 first!\"\n",
    "\n",
    "# --- 1. Enhanced Nuclear Reset Option ---\n",
    "def reset_chroma_store():\n",
    "    \"\"\"Completely wipe and recreate the vector store with thorough cleanup\"\"\"\n",
    "    try:\n",
    "        # Step 1: Close and delete all existing Chroma clients\n",
    "        if 'client' in globals():\n",
    "            try:\n",
    "                client.reset()  # Reset the client\n",
    "                del globals()['client']  # Remove from globals\n",
    "            except Exception as e:\n",
    "                print(f\"Error closing client: {e}\")\n",
    "        \n",
    "        # Step 2: Force garbage collection to release memory\n",
    "        gc.collect()\n",
    "\n",
    "        # Step 3: Kill ChromaDB server processes (Windows-specific)\n",
    "        os.system(\"taskkill /f /im chromadb-server.exe 2> nul\")\n",
    "        time.sleep(5)  # Increased wait time for process termination\n",
    "\n",
    "        # Step 4: Remove directory with retries\n",
    "        store_path = \"../vector_store\"\n",
    "        if os.path.exists(store_path):\n",
    "            for attempt in range(5):\n",
    "                try:\n",
    "                    shutil.rmtree(store_path)\n",
    "                    print(f\"Successfully removed {store_path}\")\n",
    "                    break\n",
    "                except PermissionError as pe:\n",
    "                    print(f\"PermissionError on attempt {attempt + 1}: {pe}. Retrying...\")\n",
    "                    time.sleep(3)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error removing directory on attempt {attempt + 1}: {e}\")\n",
    "                    time.sleep(3)\n",
    "            else:\n",
    "                raise Exception(f\"Failed to remove {store_path} after 5 attempts\")\n",
    "\n",
    "        # Step 5: Create fresh directory\n",
    "        os.makedirs(store_path, exist_ok=True)\n",
    "        print(f\"Created fresh directory at {store_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Reset error: {e}\")\n",
    "        raise\n",
    "\n",
    "# --- 2. Force Fresh Start ---\n",
    "try:\n",
    "    reset_chroma_store()\n",
    "except Exception as e:\n",
    "    print(f\"Reset failed: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- 3. Initialize with Clean Settings ---\n",
    "try:\n",
    "    # Use absolute path to avoid relative path issues\n",
    "    store_path = os.path.abspath(\"../vector_store\")\n",
    "    print(f\"Initializing Chroma client with path: {store_path}\")\n",
    "    \n",
    "    client = chromadb.PersistentClient(\n",
    "        path=store_path,\n",
    "        settings=chromadb.Settings(\n",
    "            allow_reset=True,\n",
    "            is_persistent=True,\n",
    "            persist_directory=store_path,\n",
    "            anonymized_telemetry=False  # Explicitly disable telemetry to avoid settings mismatch\n",
    "        )\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Client initialization failed: {e}\")\n",
    "    print(\"Attempting reset and retry...\")\n",
    "    reset_chroma_store()\n",
    "    client = chromadb.PersistentClient(\n",
    "        path=store_path,\n",
    "        settings=chromadb.Settings(\n",
    "            allow_reset=True,\n",
    "            is_persistent=True,\n",
    "            persist_directory=store_path,\n",
    "            anonymized_telemetry=False\n",
    "        )\n",
    "    )\n",
    "\n",
    "# --- 4. Create Collection ---\n",
    "try:\n",
    "    # Explicitly delete any existing collection\n",
    "    try:\n",
    "        client.delete_collection(\"complaints\")\n",
    "        print(\"Deleted existing 'complaints' collection\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    collection = client.get_or_create_collection(\n",
    "        name=\"complaints\",\n",
    "        metadata={\"hnsw:space\": \"cosine\"}\n",
    "    )\n",
    "    print(\"Successfully created or retrieved 'complaints' collection\")\n",
    "except Exception as e:\n",
    "    print(f\"Collection creation failed: {e}\")\n",
    "    reset_chroma_store()\n",
    "    collection = client.create_collection(\n",
    "        name=\"complaints\",\n",
    "        metadata={\"hnsw:space\": \"cosine\"}\n",
    "    )\n",
    "\n",
    "# --- 5. Batch Insert with Error Handling ---\n",
    "batch_size = 1000  # Reduced for stability\n",
    "successful_batches = 0\n",
    "\n",
    "with tqdm(total=len(chunks)) as pbar:\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        try:\n",
    "            batch_ids = [str(j) for j in range(i, min(i + batch_size, len(chunks)))]\n",
    "            batch_texts = [chunks[j].page_content for j in range(i, min(i + batch_size, len(chunks)))]\n",
    "            batch_embeddings = embeddings[i:i + batch_size].tolist()\n",
    "            \n",
    "            collection.add(\n",
    "                ids=batch_ids,\n",
    "                documents=batch_texts,\n",
    "                embeddings=batch_embeddings\n",
    "            )\n",
    "            successful_batches += 1\n",
    "            pbar.update(len(batch_ids))\n",
    "        except Exception as e:\n",
    "            print(f\"\\nBatch {i//batch_size} failed: {str(e)[:200]}...\")\n",
    "            time.sleep(5)\n",
    "            continue\n",
    "\n",
    "print(f\"‚úÖ Completed {successful_batches} batches. Vector store ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cffb9ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded collection with 825338 entries\n",
      "\n",
      "‚ÑπÔ∏è Collection contains 825338 entries\n",
      "‚ÑπÔ∏è Query embedding shape: 384 dimensions\n",
      "\n",
      "üîç Top 5 matches for 'late delivery complaint':\n",
      "\n",
      "#1 (Score: 0.526):\n",
      "late fee\n",
      "\n",
      "#2 (Score: 0.526):\n",
      "late fee\n",
      "\n",
      "#3 (Score: 0.526):\n",
      "late fee\n",
      "\n",
      "#4 (Score: 0.526):\n",
      "late fee\n",
      "\n",
      "#5 (Score: 0.526):\n",
      "late fee\n",
      "\n",
      "‚ÑπÔ∏è Collection contains 825338 entries\n",
      "‚ÑπÔ∏è Query embedding shape: 384 dimensions\n",
      "\n",
      "üîç Top 5 matches for 'package never arrived':\n",
      "\n",
      "#1 (Score: 0.369):\n",
      "problem with customer service\n",
      "\n",
      "#2 (Score: 0.369):\n",
      "problem with customer service\n",
      "\n",
      "#3 (Score: 0.369):\n",
      "problem with customer service\n",
      "\n",
      "#4 (Score: 0.226):\n",
      "other service problem\n",
      "\n",
      "#5 (Score: 0.226):\n",
      "other service problem\n",
      "\n",
      "‚ÑπÔ∏è Collection contains 825338 entries\n",
      "‚ÑπÔ∏è Query embedding shape: 384 dimensions\n",
      "\n",
      "üîç Top 5 matches for 'refund not processed':\n",
      "\n",
      "#1 (Score: 0.469):\n",
      "charged fees or interest you didnt expect\n",
      "\n",
      "#2 (Score: 0.469):\n",
      "charged fees or interest you didnt expect\n",
      "\n",
      "#3 (Score: 0.469):\n",
      "charged fees or interest you didnt expect\n",
      "\n",
      "#4 (Score: 0.469):\n",
      "charged fees or interest you didnt expect\n",
      "\n",
      "#5 (Score: 0.469):\n",
      "charged fees or interest you didnt expect\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ids': [['715264', '715327', '741908', '742099', '742973']],\n",
       " 'embeddings': None,\n",
       " 'documents': [['charged fees or interest you didnt expect',\n",
       "   'charged fees or interest you didnt expect',\n",
       "   'charged fees or interest you didnt expect',\n",
       "   'charged fees or interest you didnt expect',\n",
       "   'charged fees or interest you didnt expect']],\n",
       " 'uris': None,\n",
       " 'included': ['documents', 'distances'],\n",
       " 'data': None,\n",
       " 'metadatas': None,\n",
       " 'distances': [[0.5307751297950745,\n",
       "   0.5307751297950745,\n",
       "   0.5307751297950745,\n",
       "   0.5307751297950745,\n",
       "   0.5307751297950745]]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import chromadb\n",
    "import os\n",
    "import psutil\n",
    "from chromadb.utils import embedding_functions\n",
    "import time\n",
    "\n",
    "# --- 1. Nuclear Cleanup ---\n",
    "def kill_chroma_processes():\n",
    "    \"\"\"Force kill all Chroma-related processes\"\"\"\n",
    "    for proc in psutil.process_iter(['name']):\n",
    "        if 'chroma' in proc.info['name'].lower():\n",
    "            try:\n",
    "                proc.kill()\n",
    "            except:\n",
    "                pass\n",
    "    time.sleep(2)  # Wait for cleanup\n",
    "\n",
    "kill_chroma_processes()\n",
    "\n",
    "# --- 2. Initialize with EXACT Task 2 Settings ---\n",
    "store_path = os.path.abspath(\"../vector_store\")\n",
    "client = chromadb.PersistentClient(\n",
    "    path=store_path,\n",
    "    settings=chromadb.Settings(\n",
    "        allow_reset=True,\n",
    "        anonymized_telemetry=False,\n",
    "        is_persistent=True\n",
    "    )\n",
    ")\n",
    "\n",
    "# --- 3. Load Collection (No Embedding Function) ---\n",
    "try:\n",
    "    collection = client.get_collection(\"complaints\")\n",
    "    print(f\"‚úÖ Loaded collection with {collection.count()} entries\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Collection error: {e}\")\n",
    "    print(\"Available collections:\", [col.name for col in client.list_collections()])\n",
    "    raise\n",
    "\n",
    "# --- 4. Query Using Manual Embeddings ---\n",
    "def query_complaints(search_text: str, n_results=5):\n",
    "    try:\n",
    "        # 1. Verify collection is properly loaded\n",
    "        print(f\"\\n‚ÑπÔ∏è Collection contains {collection.count()} entries\")\n",
    "        \n",
    "        # 2. Initialize embedder (MUST match Task 2)\n",
    "        embedder = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "            \"all-MiniLM-L6-v2\"\n",
    "        )\n",
    "        \n",
    "        # 3. Generate query embedding\n",
    "        query_embedding = embedder([search_text])\n",
    "        print(f\"‚ÑπÔ∏è Query embedding shape: {len(query_embedding[0])} dimensions\")\n",
    "        \n",
    "        # 4. Basic query without filters first\n",
    "        results = collection.query(\n",
    "            query_embeddings=query_embedding,\n",
    "            n_results=n_results,\n",
    "            include=[\"documents\", \"distances\"]\n",
    "        )\n",
    "        \n",
    "        # 5. Verify results\n",
    "        if not results['documents'][0]:\n",
    "            print(\"‚ö†Ô∏è No results found. Trying fallback methods...\")\n",
    "            \n",
    "            # Fallback 1: Try without embeddings\n",
    "            results = collection.query(\n",
    "                query_texts=[search_text],\n",
    "                n_results=n_results\n",
    "            )\n",
    "            \n",
    "            # Fallback 2: Show random samples if still empty\n",
    "            if not results['documents'][0]:\n",
    "                samples = collection.peek()['documents']\n",
    "                print(\"\\n‚ÑπÔ∏è Sample documents in collection:\")\n",
    "                for i, doc in enumerate(samples[:3]):\n",
    "                    print(f\"{i+1}. {doc[:100]}...\")\n",
    "                return None\n",
    "        \n",
    "        # 6. Display results\n",
    "        print(f\"\\nüîç Top {n_results} matches for '{search_text}':\")\n",
    "        for idx, (doc, dist) in enumerate(zip(results[\"documents\"][0], results[\"distances\"][0])):\n",
    "            print(f\"\\n#{idx+1} (Score: {1-dist:.3f}):\")\n",
    "            print(doc[:500] + (\"...\" if len(doc) > 500 else \"\"))\n",
    "            \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Query failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# # Test query\n",
    "# query_complaints(\"late delivery\")\n",
    "\n",
    "# Test with different queries\n",
    "query_complaints(\"late delivery complaint\")\n",
    "query_complaints(\"package never arrived\")\n",
    "query_complaints(\"refund not processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5207e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Simbo\\Desktop\\week6\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚û°Ô∏è Loading data...\n",
      "Raw CSV contains 825338 rows\n",
      "\n",
      "üîç Data sample (first 3 rows):\n",
      "                                                       clean_text\n",
      "0  problem with a companys investigation into an existing problem\n",
      "1                                             managing an account\n",
      "2                                              closing an account\n",
      "\n",
      "After removing empty texts: 825338\n",
      "\n",
      "üöÄ Testing without deduplication...\n",
      "First 3 sample texts:\n",
      "1. problem with a companys investigation into an existing problem\n",
      "2. managing an account\n",
      "3. closing an account\n",
      "\n",
      "Generated 1000 chunks from first 1000 texts\n",
      "Sample chunk: problem with a companys investigation into an existing problem...\n",
      "\n",
      "‚ú® Proceeding with full processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 825338/825338 [00:34<00:00, 24099.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total chunks generated: 825338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 826/826 [1:03:36<00:00,  4.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Final collection count: 825338\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# 1. Load data with validation\n",
    "print(\"‚û°Ô∏è Loading data...\")\n",
    "data = pd.read_csv('../data/filtered_complaints.csv')\n",
    "print(f\"Raw CSV contains {len(data)} rows\")\n",
    "\n",
    "# Basic data validation\n",
    "print(\"\\nüîç Data sample (first 3 rows):\")\n",
    "print(data[['clean_text']].head(3).to_string())\n",
    "\n",
    "# 2. Extract texts with null checking\n",
    "texts = data['clean_text'].dropna().astype(str).tolist()\n",
    "print(f\"\\nAfter removing empty texts: {len(texts)}\")\n",
    "\n",
    "# 3. Minimal deduplication (temporarily disabled)\n",
    "print(\"\\nüöÄ Testing without deduplication...\")\n",
    "sample_texts = texts[:1000]  # Start with smaller sample for debugging\n",
    "print(f\"First 3 sample texts:\\n1. {sample_texts[0]}\\n2. {sample_texts[1]}\\n3. {sample_texts[2]}\")\n",
    "\n",
    "# 4. Chunking test\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \" \", \"\"]\n",
    ")\n",
    "\n",
    "test_chunks = splitter.create_documents(sample_texts)\n",
    "print(f\"\\nGenerated {len(test_chunks)} chunks from first 1000 texts\")\n",
    "print(\"Sample chunk:\", test_chunks[0].page_content[:100] + \"...\")\n",
    "\n",
    "# 5. Full processing (only proceed if test looks good)\n",
    "if len(test_chunks) > 50:  # Sanity check\n",
    "    print(\"\\n‚ú® Proceeding with full processing...\")\n",
    "    chunks = []\n",
    "    for text in tqdm(texts, desc=\"Chunking\"):  # Process all texts\n",
    "        chunks.extend(splitter.create_documents([text]))\n",
    "    \n",
    "    print(f\"\\nTotal chunks generated: {len(chunks)}\")\n",
    "    \n",
    "    # Vector store creation\n",
    "    store_path = \"../vector_store\"\n",
    "    if os.path.exists(store_path):\n",
    "        shutil.rmtree(store_path)\n",
    "    \n",
    "    client = chromadb.PersistentClient(path=store_path)\n",
    "    embedder = embedding_functions.SentenceTransformerEmbeddingFunction(\"all-MiniLM-L6-v2\")\n",
    "    \n",
    "    collection = client.create_collection(\n",
    "        name=\"complaints\",\n",
    "        embedding_function=embedder,\n",
    "        metadata={\"hnsw:space\": \"cosine\"}\n",
    "    )\n",
    "    \n",
    "    # Batch insert\n",
    "    batch_size = 1000\n",
    "    for i in tqdm(range(0, len(chunks), batch_size), desc=\"Inserting\"):\n",
    "        batch = chunks[i:i+batch_size]\n",
    "        collection.add(\n",
    "            documents=[chunk.page_content for chunk in batch],\n",
    "            ids=[str(j) for j in range(i, i+len(batch))]\n",
    "        )\n",
    "    \n",
    "    print(f\"\\n‚úÖ Final collection count: {collection.count()}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Insufficient chunks generated. Please check:\")\n",
    "    print(\"1. Are texts being properly split?\")\n",
    "    print(\"2. Is the CSV format correct?\")\n",
    "    print(\"3. Are texts being truncated too aggressively?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46896165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection contains 825338 documents\n",
      "Sample documents:\n",
      "{'ids': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], 'embeddings': array([[-0.08353022,  0.0883428 ,  0.02909802, ..., -0.01343659,\n",
      "         0.06147716,  0.03614562],\n",
      "       [-0.00793637, -0.0282068 , -0.05061107, ...,  0.02778311,\n",
      "        -0.00288957, -0.05993639],\n",
      "       [ 0.03076069, -0.00791154, -0.00195179, ...,  0.00922667,\n",
      "        -0.01095176, -0.08378277],\n",
      "       ...,\n",
      "       [-0.06894812, -0.0487364 , -0.02612704, ..., -0.00656051,\n",
      "        -0.0271311 , -0.00746608],\n",
      "       [ 0.03076069, -0.00791154, -0.00195179, ...,  0.00922667,\n",
      "        -0.01095176, -0.08378277],\n",
      "       [-0.02496498,  0.01398851, -0.01496419, ...,  0.02838119,\n",
      "         0.05266776, -0.04888583]], shape=(10, 384)), 'documents': ['problem with a companys investigation into an existing problem', 'managing an account', 'closing an account', 'problem with a companys investigation into an existing problem', 'problem with a companys investigation into an existing problem', 'problem with a companys investigation into an existing problem', 'incorrect information on your report', 'other service problem', 'closing an account', 'getting a credit card'], 'uris': None, 'included': ['metadatas', 'documents', 'embeddings'], 'data': None, 'metadatas': [None, None, None, None, None, None, None, None, None, None]}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Collection contains {collection.count()} documents\")\n",
    "print(\"Sample documents:\")\n",
    "print(collection.peek())  # Verify content looks correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5aa68dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1 chunks:\n",
      "\n",
      "Chunk 1:\n",
      "This is a test complaint. It has multiple sentences. Each should become a separate chunk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\!'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\?'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\!'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\?'\n",
      "C:\\Users\\Simbo\\AppData\\Local\\Temp\\ipykernel_71348\\4227945867.py:7: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \"(?<=\\! )\", \"(?<=\\? )\", \" \", \"\"],  # Added regex lookbehinds\n",
      "C:\\Users\\Simbo\\AppData\\Local\\Temp\\ipykernel_71348\\4227945867.py:7: SyntaxWarning: invalid escape sequence '\\!'\n",
      "  separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \"(?<=\\! )\", \"(?<=\\? )\", \" \", \"\"],  # Added regex lookbehinds\n",
      "C:\\Users\\Simbo\\AppData\\Local\\Temp\\ipykernel_71348\\4227945867.py:7: SyntaxWarning: invalid escape sequence '\\?'\n",
      "  separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \"(?<=\\! )\", \"(?<=\\? )\", \" \", \"\"],  # Added regex lookbehinds\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 1. Use these optimized splitting parameters\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,          # Reduced from 300\n",
    "    chunk_overlap=30,        # Reduced from 50  \n",
    "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \"(?<=\\! )\", \"(?<=\\? )\", \" \", \"\"],  # Added regex lookbehinds\n",
    "    keep_separator=True      # Keep the punctuation\n",
    ")\n",
    "\n",
    "# 2. Test with your sample\n",
    "test_text = \"This is a test complaint. It has multiple sentences. Each should become a separate chunk.\"\n",
    "test_docs = splitter.create_documents([test_text])\n",
    "\n",
    "print(f\"Generated {len(test_docs)} chunks:\")\n",
    "for i, doc in enumerate(test_docs):\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a487e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 1 chunks:\n",
      "\n",
      "Chunk 1:\n",
      "This is a test complaint. It has multiple sentences! Each should become a separate chunk?\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import re\n",
    "\n",
    "# 1. First split into sentences using regex\n",
    "def split_into_sentences(text):\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)  # Split after punctuation\n",
    "    return [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "# 2. Then use this as the first splitting stage\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=30,\n",
    "    separators=[\n",
    "        \"\\n\\n\",  # Paragraphs first\n",
    "        \"\\n\",    # Then lines\n",
    "        \"(?<=[.!?]) +\",  # Then sentences (notice the space after)\n",
    "        \" \",      # Then words\n",
    "        \"\"        # Final fallback\n",
    "    ],\n",
    "    keep_separator=True\n",
    ")\n",
    "\n",
    "# 3. Test with your sample\n",
    "test_text = \"This is a test complaint. It has multiple sentences! Each should become a separate chunk?\"\n",
    "test_docs = splitter.create_documents([test_text])\n",
    "\n",
    "print(f\"Generated {len(test_docs)} chunks:\")\n",
    "for i, doc in enumerate(test_docs):\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5549a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a test complaint.', 'It has multiple sentences!', 'Each should become a separate chunk?']\n"
     ]
    }
   ],
   "source": [
    "print(split_into_sentences(test_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
